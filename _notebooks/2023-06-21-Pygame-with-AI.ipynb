{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Pygame with AI\"\n",
    "> \"Using language models in a pygame-based video game!\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Christina Kampel\n",
    "- categories: [python, pygame, HuggingFace, transformers, pipelines, natural language processing, NLP, machine learning, ML, artificial intelligence, AI, conversational models, question-answering models, fill-mask, text-generation]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on the Hard Parts\n",
    "\n",
    "This tutorial follows the third principle of [David Perkins' Seven Principles of Teaching](https://www.gse.harvard.edu/news/uk/09/01/education-bat-seven-principles-educators): Work on the Hard Parts. Here, we will become familiar with the HuggingFace library and implement a pre-trained machine learning model into a pygame-based video game. We will then improve our skills by practicing with three more types of models. By the end of this tutorial, you should feel confident exploring the HuggingFace library on your own."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation Requirements\n",
    "\n",
    "It is recommended to set up a virtual environment for the installations below. See [Installing packages using pip and virtual environments](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/).\n",
    "\n",
    "To ensure all libraries are installed correctly, see the [HuggingFace Quicktour](https://huggingface.co/docs/transformers/quicktour).\n",
    "\n",
    "| Installation | Version | Links |\n",
    "| ------------ | ------- | ----- |\n",
    "| Python | 3.9.13 or above | [Python Downloads page](https://www.python.org/downloads/) |\n",
    "| Pygame | 2.4.0 or above | [Pygame Getting Started wiki](https://www.pygame.org/wiki/GettingStarted) |\n",
    "| Pytorch | 2.0.1 with computing platform CUDA 11.8 | [Pytorch website](https://pytorch.org/) |\n",
    "| Requests | 2.31.0 or above | [Installing Packages](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/) |\n",
    "| HuggingFace Transformers | 4.29.2 or above | [Transformers installation](https://huggingface.co/docs/transformers/installation) |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try This Out!\n",
    "\n",
    "This game uses the models in this tutorial to power non-player characters the player can talk to: [ai_game.py](https://github.com/de-fellows/Intro-to-Pygame-and-AI/blob/main/ai_game.py)\n",
    "\n",
    "To see the game above without AI models, check out this simple pygame example: [simple_pygame.py](https://github.com/de-fellows/Intro-to-Pygame-and-AI/blob/main/simple_pygame.py)\n",
    "\n",
    "To learn about the basics of pygame, check out this blog post: [Intro to Pygame: Pygame basics for your first video game!](https://de-fellows.github.io/RexCoding/python/pygame/2023/06/21/Intro-to-Pygame.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace and Natural Language Processing\n",
    "\n",
    "The goals of this tutorial are to:\n",
    "- Explore a variety of language models from the [HuggingFace](https://huggingface.co/) library\n",
    "- Load the models into a pygame-based video game\n",
    "- Use the models to generate text for non-player characters (NPCs) that a player can interact with\n",
    "\n",
    "Natural Language Processing (NLP) is when a machine learning model is trained and used on linguistic data to achieve a task. Tasks may include text classification (assigning a label to text), question answering, text generation. HuggingFace is a great source for all kinds of models and datasets, including those for NLP."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "There are two main ways to use a publicly-available model:\n",
    "\n",
    "1. **The Slow Way - Manually loading a model and tokenizer into variables.** This requires encoding (converting to numeric values) text data before it can be inputted into the model, and decoding the model's output.\n",
    "2. **The Fast Way - Pipelines.** The HuggingFace `pipeline()` function is a wrapper for models that automatically encodes and decodes data. It also allows a `task` to be specified, a.k.a. what you want the model to do. Each task has an out-of-the-box default model and tokenizer, or a model can be specified. See the [pipeline API reference](https://huggingface.co/docs/transformers/main_classes/pipelines) for more information.\n",
    "\n",
    "The section below will use both methods to implement a conversational NLP model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational Models\n",
    "\n",
    "\"Conversational response modelling is **the task of generating conversational text that is relevant, coherent and knowledgeable given a prompt.** These models have applications in chatbots, and as a part of voice assistants.\" - [HuggingFace Guide on Conversational NLP Tasks](https://huggingface.co/tasks/conversational).\n",
    "\n",
    "In our video game, the goal is to make a character that the player can chat back-and-forth with. We will use a conversational model to do so."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Slow Way - Manually Loading a Model and Tokenizer\n",
    "\n",
    "First, we need to load the conversational model [facebook/blenderbot-400M-distill](https://huggingface.co/facebook/blenderbot-400M-distill?text=Hi.) into a `tokenizer` and a `model`:\n",
    "\n",
    "- The `tokenizer` takes text data and turns it into a list of numbers (`tokens`), where each token represents a certain word or character. This step is needed for the model to process the data.\n",
    "- The `model` takes a tokenized input and generates a response that is also tokenized. This response must be decoded (converted from numbers into words) using the `tokenizer`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Models can be found at https://huggingface.co/models. On each model's page, look for the \"</> Use in Transformers\" button for the code needed to initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a chatbot with the model facebook/blenderbot-400M-distill\n",
    "# code to initalize model found at: https://huggingface.co/facebook/blenderbot-400M-distill?text=Hi.\n",
    "\n",
    "# import libraries\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# set up tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/blenderbot-400M-distill\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a sentence for the model to respond to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is your favourite colour?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a sentence for our model to respond to\n",
    "utterance = \"What is your favourite colour?\"\n",
    "utterance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the sentence into a format the model can process (PyTorch tensors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 714,  315,  414, 6179, 7796,   38,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return_rensors='pt' makes the inputs into pytorch tensors\n",
    "# otherwise, the tokenizer will return lists\n",
    "inputs = tokenizer(utterance, return_tensors='pt')\n",
    "inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer has encoded the sentence into `input_ids`. Note that the end of the sequence, <\\/s>, is its own character. Ignore `attention_mask` for now.\n",
    "\n",
    "| Word | input_id |\n",
    "| ---- | -------- |\n",
    "| 'What' | 714 |\n",
    "| 'is' | 315 |\n",
    "| 'your' | 414 |\n",
    "| 'favourite' | 6179 |\n",
    "| 'colour' | 7796 |\n",
    "| '?' | 38 |\n",
    "| '<\\/s>' | 2 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the tokenizer as a decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' What is your favourite colour?</s>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode the sentence we just encoded\n",
    "tokenizer.decode(inputs.input_ids[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the model's response to our question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christina\\Desktop\\Python\\Digital Engineering Fellowship 2023\\Christina-Kampel-Draft-2023\\ai-game-env\\lib\\site-packages\\transformers\\generation\\utils.py:1346: UserWarning: Using `max_length`'s default (60) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[   1,  863, 2297, 3183,  315, 3002,   21,  228,  714,  315, 4228,   38,\n",
       "          228,  946,  304,  360,  265, 2297, 3183,   38,    2]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unpack (**) the inputs variable into the model\n",
    "response = model.generate(**inputs)\n",
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode the response into words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> My favorite color is blue.  What is yours?  Do you have a favorite color?</s>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since the data is in double braces, we need to use [0] to access the encoded data\n",
    "tokenizer.decode(response[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding and Attention Masks\n",
    "\n",
    "Above, we gave the model one sentence to respond to. What if we want to give it a batch of a few sentences? Then, we need to do things:\n",
    "\n",
    "1. Make all of the encoded tensors the same length by **padding** them (adding a token to make all of the encoded sentences the same length).\n",
    "2. Give the model an **attention mask** - a tensor that tells the model which tokens are important and which tokens are padding.\n",
    "\n",
    "For more information on padding and attention masks, see: https://lukesalamone.github.io/posts/what-are-attention-masks/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a batch of sentences for the model to respond to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a batch of sentences for our model to respond to\n",
    "utterance_batch = [\"What is your favourite colour?\",\n",
    "                   \"I like coding. What do you like to do?\",\n",
    "                   \"What time is dinner?\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can control the direction that the padding tokens are applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell the tokenizer to pad from the left\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify which token is used for padding. This is not always needed. Here, we are using the \"end of sequence\" token for padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the updated tokenizer to encode the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   2,    2,    2,    2,    2,    2,  714,  315,  414, 6179, 7796,   38,\n",
       "            2],\n",
       "        [ 281,  398, 6601,  278,   21,  714,  361,  304,  398,  287,  361,   38,\n",
       "            2],\n",
       "        [   2,    2,    2,    2,    2,    2,    2,  714,  552,  315, 5048,   38,\n",
       "            2]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode the batch\n",
    "input_batch = tokenizer(utterance_batch, return_tensors='pt', padding=True)\n",
    "input_batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice:\n",
    "- the padding token is `2`, which appears on the left of each tensor\n",
    "- each tensor in `input_ids` has a corresponding tensor in `attention_mask` (see below)\n",
    "\n",
    "The attention mask tells the model if a token in `input_ids` is important (`1`), or is a padding value, and therefore not important (`0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids = tensor([   2,    2,    2,    2,    2,    2,  714,  315,  414, 6179, 7796,   38,\n",
      "           2])\n",
      "attention_mask = tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# For easy reading, show the data for only the first sentence in the batch\n",
    "\n",
    "# encoded ids\n",
    "first_sentence_ids = input_batch.input_ids[0]\n",
    "# attention mask\n",
    "first_sentence_mask = input_batch.attention_mask[0]\n",
    "print(f\"input_ids = {first_sentence_ids}\\nattention_mask = {first_sentence_mask}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's pass the entire batch to the model and get its responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,  863, 2297, 3183,  315, 3002,   21,  228,  714,  315, 4228,   38,\n",
       "          228,  946,  304,  360,  265, 2297, 3183,   38,    2,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,  281,  398,  287,  525, 1620, 1012,  298, 1484, 2842,   21,  714,\n",
       "          906,  306, 6601,  278,  361,  304,  361,   38,  228,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    1,  417,  267, 1336,  315,  403, 1226,   33, 2527,   21,  228,\n",
       "          281,  632,  655,  287,  627,  265,  893, 1718,  306,  508,  558, 2595,\n",
       "           91,   80,  298, 3597, 1884,   90,   21,    2]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, unpacking (**) is important because it gives the model the attention_mask\n",
    "response_batch = model.generate(**input_batch)\n",
    "response_batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode the responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> My favorite color is blue.  What is yours?  Do you have a favorite color?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<s> I like to play video games and watch movies. What kind of coding do you do? </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<s><s> Dinner is at 8:30.  I am going to make a big pot of spaghetti and meatballs.</s>\n"
     ]
    }
   ],
   "source": [
    "for item in response_batch:\n",
    "    print(tokenizer.decode(item))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Fast Way - Pipelines\n",
    "\n",
    "The `pipeline` transformer performs the same tasks as above, but automatically encodes and decodes text!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the same model and tokenizer as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the model and tokenizer (same as before)\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/blenderbot-400M-distill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the tokenizer to left padding using the eos token (same as before)\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `pipeline` wrapper on the `model` and `tokenizer` to create a chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# the chatbot - since task=\"conversational\", pipline returns a ConversationalPipeline\n",
    "blenderbot = pipeline(task=\"conversational\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chatbot is a `ConversationalPipeline` object, which accepts a `Conversation` object as its input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conversation id: 100 \n",
       "user >> Hi. How are you? "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Conversation\n",
    "\n",
    "# start a conversation with a chatbot - no need for encoding!\n",
    "# conversation_id is manually set for reproducibility\n",
    "# if conversation_id is not set, an id is randomly generated\n",
    "conversation = Conversation(\"Hi. How are you?\", conversation_id=\"100\")\n",
    "conversation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our conversation has unprocessed user input, so we can pass it to the chatbot to get a response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conversation id: 100 \n",
       "user >> Hi. How are you? \n",
       "bot >>  I'm doing well, thank you. How about yourself? Do you have any plans for the weekend? "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the bot's response\n",
    "blenderbot(conversation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bot's response has been appended to the `Conversation` object! This way, the object stores the conversation history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conversation id: 100 \n",
       "user >> Hi. How are you? \n",
       "bot >>  I'm doing well, thank you. How about yourself? Do you have any plans for the weekend? "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the updated conversation (chat history)\n",
    "conversation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `past_user_inputs` attribute returns a list of everything the user said:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi. How are you?']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.past_user_inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generated_responses` attribute returns a list of everything the bot said:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" I'm doing well, thank you. How about yourself? Do you have any plans for the weekend?\"]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.generated_responses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `add_user_input()` method allows us to add new user input to the conversation. Note that the chatbot can only respond to conversations that have unprocessed user input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conversation id: 100 \n",
       "user >> Hi. How are you? \n",
       "bot >>  I'm doing well, thank you. How about yourself? Do you have any plans for the weekend? \n",
       "user >> What do you want to do this weekend? "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add user input\n",
    "conversation.add_user_input(\"What do you want to do this weekend?\")\n",
    "conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conversation id: 100 \n",
       "user >> Hi. How are you? \n",
       "bot >>  I'm doing well, thank you. How about yourself? Do you have any plans for the weekend? \n",
       "user >> What do you want to do this weekend? \n",
       "bot >>  I'm going to a concert with some friends. I've never been to one before. "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chatbot responds to the new input\n",
    "blenderbot(conversation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimming a Conversation\n",
    "\n",
    "The \"Conversation input is too long\" warning may appear after only a few back-and-forth exchanges. The pipeline automatically trims the input, but manual trimming is also an option. This is useful if you only want to show the most recent few lines of a conversation, not the entire chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define trimming function\n",
    "def trim_convo(conversation):\n",
    "    \"\"\"Trim the earliest user and bot lines from a Conversation.\n",
    "\n",
    "    Parameters:\n",
    "    - conversation (transformers.pipelines.conversational.Conversation object): conversation to trim\n",
    "\n",
    "    Returns:\n",
    "    - Trimmed conversation (transformers.pipelines.conversational.Conversation object)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conversation.past_user_inputs.pop(0)\n",
    "        conversation.generated_responses.pop(0)\n",
    "        return conversation\n",
    "    except:\n",
    "        warning = f\"Conversation is too short to be trimmed.\"\n",
    "        print(warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conversation id: 100 \n",
       "user >> What do you want to do this weekend? \n",
       "bot >>  I'm going to a concert with some friends. I've never been to one before. "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test out the function\n",
    "trim_convo(conversation=conversation)\n",
    "\n",
    "# see results\n",
    "conversation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Caveats\n",
    "\n",
    "While it is useful for producing a back-and-forth conversation, the blenderbot model does not store information from the entire conversation in its responses. For example, if you tell the blenderbot that your favourite colour is blue, and then ask it what your favourite colour is, it will not remember the answer. It may also lose the context of the conversation and give answers that are nonsensical or unrelated to the question."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question-Answering Models\n",
    "\n",
    "\"Question Answering models can **retrieve the answer to a question from a given text**, which is useful for searching for an answer in a document. Some question answering models can generate answers without context!\" - [HuggingFace Guide on Question-Answering Tasks](https://huggingface.co/tasks/question-answering)\n",
    "\n",
    "For our video game, the goal is to make a non-player-character (NPC) that can answer questions about the game. This means that context - the information the model uses in its responses - is important.\n",
    "\n",
    "For this tutorial, let's compare two question-answering models:\n",
    "- [distilbert-base-cased-distilled-squad](https://huggingface.co/distilbert-base-cased-distilled-squad): an **extractive model**, meaning that it extracts the answer out of the given context\n",
    "- [t5-base](https://huggingface.co/t5-base): a **text-to-text generation model** that has a wide range of applications such as question-answering, translating and summarizing. This model generates new text based on the given context."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extractive Model\n",
    "\n",
    "Set up the model using a `pipeline` and `task=\"question-answering\"`. Note that `distilbert-base-cased-distilled-squad` is the default model for this task, so there is no need to specify the model and tokenizer when we are just testing the model out.\n",
    "\n",
    "> Note: In production, it's good practice to specify the model and tokenizer as was done for the Conversational Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christina\\Desktop\\Python\\Digital Engineering Fellowship 2023\\Christina-Kampel-Draft-2023\\ai-game-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from transformers import pipeline\n",
    "\n",
    "# set up model\n",
    "qa_model = pipeline(task=\"question-answering\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give the model a question and some context, and see what it's response is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.2849337160587311,\n",
       " 'start': 14,\n",
       " 'end': 33,\n",
       " 'answer': 'the top of the tree'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up question and context\n",
    "question = \"Where is the key?\"\n",
    "context = \"The key is at the top of the tree.\"\n",
    "\n",
    "# get model's response\n",
    "qa_model(question=question, context=context)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-to-Text Generation Model\n",
    "\n",
    "Set up the model using a `pipeline` and `task=\"text2text-generation\"`. Note that `t5-base` is the default model for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\Users\\Christina\\Desktop\\Python\\Digital Engineering Fellowship 2023\\Christina-Kampel-Draft-2023\\ai-game-env\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "t2t_model = pipeline(task=\"text2text-generation\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give the model a question and some context. For this model, the \"question\" and \"context\" labels are used inside a string as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'the top of the tree'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get model's response to the question, given the context\n",
    "t2t_model(\"question: Where can I find the key? context: The key is at the top of the tree.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding to the Context\n",
    "\n",
    "So far, the two models have given the same responses. To test out their differences, let's give the models a larger chunk of information as its context so it can answer a wider range of questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lots of context\n",
    "context_large = \"\"\"This game has the following objects in it: Player Bear, Wall, Tree, Key, Lock and Polar Bear.\n",
    "                The Player Bear is a character controlled by you, the user. You can use the arrow keys to make the Player Bear move around,\n",
    "                 and the RETURN or ENTER keys to talk with other chatbots. The wall is an impassible obstacle.\n",
    "                  The tree and the lock are interactive objects. You can climb the tree to find the key at the top.\n",
    "                   Once you have the key, you can use the key to open or unlock the lock.\n",
    "                    You can talk to the Polar Bear as well. \"NPC\" stands for \"non-player character\". The Polar Bear is a conversational chatbot NPC that uses the\n",
    "                     facebook/blenderbot-400M-distill model. \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the models' responses to the same questions:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** How do I move around?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.5670624375343323, 'start': 186, 'end': 196, 'answer': 'arrow keys'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extractive model\n",
    "qa_model(question=\"How do I move around?\", context=context_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'arrow keys'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text-to-text model\n",
    "t2t_model(f\"question: How do I move around? context: {context_large}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** How do I get to the key?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.3596245348453522,\n",
       " 'start': 526,\n",
       " 'end': 549,\n",
       " 'answer': 'open or unlock the lock'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extractive model\n",
    "qa_model(question=\"How do I get to the key?\", context=context_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'climb the tree'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text-to-text model\n",
    "t2t_model(f\"question: How do I get to the key? context: {context_large}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** How many bears are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.3472032845020294,\n",
       " 'start': 43,\n",
       " 'end': 92,\n",
       " 'answer': 'Player Bear, Wall, Tree, Key, Lock and Polar Bear'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extractive model\n",
    "qa_model(question=\"How many bears are there?\", context=context_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Polar Bear'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text-to-text model\n",
    "t2t_model(f\"question: How many bears are there? context: {context_large}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** Who is the Polar Bear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.5299234986305237,\n",
       " 'start': 670,\n",
       " 'end': 698,\n",
       " 'answer': 'a conversational chatbot NPC'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extractive model\n",
    "qa_model(question=\"Who is the Polar Bear?\", context=context_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'conversational chatbot NPC'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text-to-text model\n",
    "t2t_model(f\"question: Who is the Polar Bear? context: {context_large}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Caveats\n",
    "\n",
    "As shown above, both models give similar answers. However, neither model can correctly answer Question 3 (\"How many bears are there?\"). The answer should be \"two\", which can be inferred from the context but is not explicitly stated. This shows that neither model is good at inferring information from the context. To solve this problem, a different model could be used, or more information could be included in the context to make answers easier for the model to find."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicating the `Conversation` Object\n",
    "\n",
    "When we put the Question-Answering model into production, we may want to store conversation data in a similar way as the `Conversation` object used for the Conversational Model. This can be done using a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a conversation dictionary to hold the chat history\n",
    "# like the Conversation object, past_user_inputs will store the user's input and generated_responses will store the chatbot's responses\n",
    "conversation2 = {\"past_user_inputs\": [], \"generated_responses\": []}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lines of text can be added using `.append()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'past_user_inputs': ['How do I get the key?'],\n",
       " 'generated_responses': [\"Hi, I'm a question-answering bot. Ask me a question!\"]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add a line to the list of chatbot's responses\n",
    "conversation2[\"generated_responses\"].append(\"Hi, I'm a question-answering bot. Ask me a question!\")\n",
    "\n",
    "# add a line to the user input\n",
    "conversation2[\"past_user_inputs\"].append(\"How do I get the key?\")\n",
    "\n",
    "# show the conversation\n",
    "conversation2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the chatbot's responses to the question, and print it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How do I get the key?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the question from the conversation history\n",
    "question = conversation2[\"past_user_inputs\"][-1]\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.35603067278862,\n",
       " 'start': 526,\n",
       " 'end': 549,\n",
       " 'answer': 'open or unlock the lock'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get chatbot's response to the question given the context\n",
    "qa_model(question=question, context=context_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'open or unlock the lock'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we only want the 'answer'\n",
    "response = qa_model(question=question, context=context_large)[\"answer\"]\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Open or unlock the lock.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format the answer so the text looks ike a sentence - capitalize the first word and add a period at the end\n",
    "response = response.capitalize() + \".\"\n",
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, add the chatbot's response to the conversation history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'past_user_inputs': ['How do I get the key?'],\n",
       " 'generated_responses': [\"Hi, I'm a question-answering bot. Ask me a question!\",\n",
       "  'Open or unlock the lock.']}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add response to conversation history\n",
    "conversation2[\"generated_responses\"].append(response)\n",
    "\n",
    "# show results\n",
    "conversation2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed, we can show the back-and-forth conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Hi, I'm a question-answering bot. Ask me a question!\n",
      "User: How do I get the key?\n",
      "Bot: Open or unlock the lock.\n"
     ]
    }
   ],
   "source": [
    "# set counters - used into index into lists\n",
    "i = 0\n",
    "j = 0\n",
    "\n",
    "while i < len(conversation2[\"generated_responses\"]):\n",
    "    # print bot response\n",
    "    print(\"Bot: \" + conversation2[\"generated_responses\"][i])\n",
    "\n",
    "    if j < len(conversation2[\"past_user_inputs\"]):\n",
    "        # print user input\n",
    "        print(\"User: \" + conversation2[\"past_user_inputs\"][j])\n",
    "    \n",
    "    # increment counters\n",
    "    i += 1\n",
    "    j += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill-Mask Models\n",
    "\n",
    "\"Masked language modeling is the task of **masking some of the words in a sentence and predicting which words should replace those masks.** These models are useful when we want to get a statistical understanding of the language in which the model is trained in.\" - [HuggingFace Guide on Fill-Mask Tasks](https://huggingface.co/tasks/fill-mask)\n",
    "\n",
    "For our video game, we will make an NPC that fills in the blanks of a sentence using a fill-mask model.\n",
    "\n",
    "We will use the [distilroberta-base](https://huggingface.co/distilroberta-base) model, the default model for `task='fill-mask'` when using a `pipeline`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up the model:\n",
    "\n",
    "> Note: Since `distilroberta-base` is the default model for this task, we do not need to specify the model and tokenizer. This is done so anyways because it is the conventional way of loading a model into production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# set up model and tokenizer\n",
    "fm_tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")\n",
    "fm_model = AutoModelForMaskedLM.from_pretrained(\"distilroberta-base\")\n",
    "\n",
    "# create chatbot\n",
    "fm_chatbot = pipeline(task=\"fill-mask\", model=fm_model, tokenizer=fm_tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test it out. Give the model a sentence containing the mask token (`\"<mask>\"`) in the place of a missing word:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: The input must contain the mask token or the pipeline will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.6790177226066589,\n",
       "  'token': 812,\n",
       "  'token_str': ' capital',\n",
       "  'sequence': 'Paris is the capital of France.'},\n",
       " {'score': 0.05177992954850197,\n",
       "  'token': 32357,\n",
       "  'token_str': ' birthplace',\n",
       "  'sequence': 'Paris is the birthplace of France.'},\n",
       " {'score': 0.03825283423066139,\n",
       "  'token': 1144,\n",
       "  'token_str': ' heart',\n",
       "  'sequence': 'Paris is the heart of France.'},\n",
       " {'score': 0.024348977953195572,\n",
       "  'token': 29778,\n",
       "  'token_str': ' envy',\n",
       "  'sequence': 'Paris is the envy of France.'},\n",
       " {'score': 0.022851353511214256,\n",
       "  'token': 1867,\n",
       "  'token_str': ' Capital',\n",
       "  'sequence': 'Paris is the Capital of France.'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input sentence with missing word\n",
    "sentence = \"Paris is the <mask> of France.\"\n",
    "\n",
    "# get the model's output\n",
    "result = fm_chatbot(\"Paris is the <mask> of France.\")\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model returned five sentences containing the five words most likely to fill in the mask."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select only certain results if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.6790177226066589,\n",
       "  'token': 812,\n",
       "  'token_str': ' capital',\n",
       "  'sequence': 'Paris is the capital of France.'},\n",
       " {'score': 0.05177992954850197,\n",
       "  'token': 32357,\n",
       "  'token_str': ' birthplace',\n",
       "  'sequence': 'Paris is the birthplace of France.'},\n",
       " {'score': 0.03825283423066139,\n",
       "  'token': 1144,\n",
       "  'token_str': ' heart',\n",
       "  'sequence': 'Paris is the heart of France.'},\n",
       " {'score': 0.024348977953195572,\n",
       "  'token': 29778,\n",
       "  'token_str': ' envy',\n",
       "  'sequence': 'Paris is the envy of France.'},\n",
       " {'score': 0.022851353511214256,\n",
       "  'token': 1867,\n",
       "  'token_str': ' Capital',\n",
       "  'sequence': 'Paris is the Capital of France.'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show all model results from the sentence \"Paris is the <mask> of France.\"\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris is the capital of France.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show only the most likely sentence - the one with the highest score\n",
    "result[0]['sequence']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also give a summary of the most likely tokens by iterating through the model's output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most likely words are: capital,  birthplace,  heart,  envy,  Capital.\n"
     ]
    }
   ],
   "source": [
    "# counter for indexing into the model's output\n",
    "i = 0\n",
    "# string to hold the most likely words\n",
    "print_string = \"\"\n",
    "\n",
    "while i < len(result):\n",
    "    # if we have reached the last word, insert a period\n",
    "    if i == len(result) - 1:\n",
    "        print_string += result[i]['token_str'] + \".\"\n",
    "    # otherwise, insert a comma and space\n",
    "    else:\n",
    "        print_string += result[i]['token_str'] + \", \"\n",
    "\n",
    "    i += 1\n",
    "\n",
    "# show results\n",
    "print(\"The most likely words are:\" + print_string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Caveats\n",
    "\n",
    "Since this model gets its data from the Internet, the output of certain phrases may include harmful stereotypes. A good example is shown in the [Bias, Risks, and Limitations](https://huggingface.co/distilroberta-base#bias-risks-and-limitations) section of the model's information page, where the creators compare the model's responses to the phrases, \"The man worked as a `<mask>`\", and, \"The woman worked as a `<mask>`\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-Generating Models\n",
    "\n",
    "\"Generating text is the task of **producing new text**. These models can, for example, fill in incomplete text or paraphrase.\" - [HuggingFace Guide on Text Generation Tasks](https://huggingface.co/tasks/text-generation)\n",
    "\n",
    "For our video game, we will use the text-generating model [gpt2](https://huggingface.co/gpt2?text=Once+upon+a+time%2C) to complete the phrase, \"Once upon a time,\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up the model and tokenizer, and pass them to the `pipeline` object along with `task=\"text-generation\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "\n",
    "# set up model and tokenizer\n",
    "tg_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tg_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# create chatbot\n",
    "tg_chatbot = pipeline(task=\"text-generation\", model=tg_model, tokenizer=tg_tokenizer, do_sample=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we set `do_sample=True`. This is not required for text generation, but it enables various decoding strategies when new text is generated. From the [HuggingFace transformers documentation](https://huggingface.co/docs/transformers/generation_strategies#:~:text=do_sample%20%3A%20if%20set%20to%20True,with%20various%20strategy%2Dspecific%20adjustments.):\n",
    "\n",
    "*\"**do_sample:** if set to True, this parameter enables decoding strategies such as multinomial sampling, beam-search multinomial sampling, Top-K sampling and Top-p sampling. All these strategies select the next token from the probability distribution over the entire vocabulary with various strategy-specific adjustments.\"*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's ask the model to complete a story starting with \"Once upon a time,\". Note that we since the generation relies on randomness, we need to set a seed for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christina\\Desktop\\Python\\Digital Engineering Fellowship 2023\\Christina-Kampel-Draft-2023\\ai-game-env\\lib\\site-packages\\transformers\\generation\\utils.py:1346: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Once upon a time, in the case of Mr. Pate, the most important part of our work, and especially when the subject of the present discussion is taken into consideration, is that the individual and the particular case are quite separate. The particular'}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the seed for reproducibility\n",
    "set_seed(50)\n",
    "# have the model fill in the story\n",
    "story = tg_chatbot(\"Once upon a time,\")\n",
    "# show results\n",
    "story"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To only see the generated text, do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Once upon a time, in the case of Mr. Pate, the most important part of our work, and especially when the subject of the present discussion is taken into consideration, is that the individual and the particular case are quite separate. The particular'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get only the string of generated text\n",
    "story_text = story[0]['generated_text']\n",
    "# show results\n",
    "story_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue the story, we can take this output and input it back into the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 50, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Once upon a time, in the case of Mr. Pate, the most important part of our work, and especially when the subject of the present discussion is taken into consideration, is that the individual and the particular case are quite separate. The particular'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the previous output as the new input for the model\n",
    "story2 = tg_chatbot(story_text)\n",
    "# get only the string of generated text\n",
    "story2_text = story2[0]['generated_text']\n",
    "# show results\n",
    "story_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened here? It looks like no new text was added.\n",
    "\n",
    "By default, the model has a `max_length` of 50 output tokens (words), including the input. To fix this, we could do one of two things:\n",
    "1. Increase `max_length`: A good short-term solution, but not useful if we want to keep expanding on the same text, since this number includes the input text.\n",
    "2. Set the `max_new_tokens`: Controls the maximum number of new words the model generates, not including the input text. A good long-term solution if we want the model to continue expanding one block of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Once upon a time, in the case of Mr. Pate, the most important part of our work, and especially when the subject of the present discussion is taken into consideration, is that the individual and the particular case are quite separate. The particular case is one which has been dealt with by the Courts so far in this Court, without reference to'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try again, but this time using max_new_tokens\n",
    "story2 = tg_chatbot(story_text, max_new_tokens=20)\n",
    "# get only the string of generated text\n",
    "story2_text = story2[0]['generated_text']\n",
    "# show results\n",
    "story2_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Caveats\n",
    "\n",
    "In production, using gpt2 to continously expand on the same block of text can result in the model giving the same output after a certain number of iterations. This may look like:\n",
    "\n",
    "```\n",
    "Iteration 1:\n",
    ">>> Input: \"Once upon a time,\"\n",
    ">>> Output: \"Once upon a time, there was a snake\"\n",
    "\n",
    "Iteration 2:\n",
    ">>> Input: \"Once upon a time, there was a snake\"\n",
    ">>> Output: \"Once upon a time, there was a snake in the garden\"\n",
    "\n",
    "Iteration 3:\n",
    ">>> Input: \"Once upon a time, there was a snake in the garden\"\n",
    ">>> Output: \"Once upon a time, there was a snake in the garden in the garden\"\n",
    "```\n",
    "\n",
    "As a result, we may need to have the option to reset the story when using this model in production."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resetting the Story\n",
    "\n",
    "To reset the story while using the same loaded model, we need to:\n",
    "1. Change the input text back to \"Once upon a time,\"\n",
    "2. Change the seed\n",
    "\n",
    "If the seed remains the same, the model will generate the same results as before.\n",
    "\n",
    "Here's proof:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "c:\\Users\\Christina\\Desktop\\Python\\Digital Engineering Fellowship 2023\\Christina-Kampel-Draft-2023\\ai-game-env\\lib\\site-packages\\transformers\\generation\\utils.py:1346: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Once upon a time, in the case of Mr. Pate, the most important part of our work, and especially when the subject of the present discussion is taken into consideration, is that the individual and the particular case are quite separate. The particular'}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restarting the story with the same model, same seed, and original input will generate the same result as before\n",
    "\n",
    "# set the seed for reproducibility\n",
    "set_seed(50)\n",
    "# have the model fill in the story\n",
    "story = tg_chatbot(\"Once upon a time,\")\n",
    "# show results\n",
    "story"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References and Companion Files\n",
    "\n",
    "**References:**\n",
    "\n",
    "- Education at Bat: Seven Principles for Educators: https://www.gse.harvard.edu/news/uk/09/01/education-bat-seven-principles-educators\n",
    "\n",
    "- HuggingFace:\n",
    "    - Website: https://huggingface.co/\n",
    "    - Quicktour: https://huggingface.co/docs/transformers/quicktour\n",
    "    - Pipelines API Reference: https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "    - Models Page: https://huggingface.co/models\n",
    "    - Transformers Documentation: https://huggingface.co/docs/transformers/generation_strategies#:~:text=do_sample%20%3A%20if%20set%20to%20True,with%20various%20strategy%2Dspecific%20adjustments.\n",
    "    \n",
    "- Conversational Models:\n",
    "    - HuggingFace Guide on Conversational NLP Tasks: https://huggingface.co/tasks/conversational\n",
    "    - facebook/blenderbot-400M-distill Model Card: https://huggingface.co/facebook/blenderbot-400M-distill?text=Hi\n",
    "    - Blenderbot tutorial video: https://www.youtube.com/watch?v=FfywuRCPmqY\n",
    "    - Blenderbot tutorial GitHub: https://github.com/nicknochnack/Blenderbot/blob/main/Blenderbot-Tutorial.ipynb \n",
    "    - What are Attention Masks? by Luke Salamone: https://lukesalamone.github.io/posts/what-are-attention-masks/\n",
    "\n",
    "- Question-Answering Models:\n",
    "    - HuggingFace Guide on Question-Answering Tasks: https://huggingface.co/tasks/question-answering\n",
    "    - distilbert-base-cased-distilled-squad Model Card: https://huggingface.co/distilbert-base-cased-distilled-squad\n",
    "    - t5-base Model Card: https://huggingface.co/t5-base\n",
    "    \n",
    "- Fill-Mask Models:\n",
    "    - HuggingFace Guide on Fill-Mask Tasks: https://huggingface.co/tasks/fill-mask\n",
    "    - distilroberta-base Model Card: https://huggingface.co/distilroberta-base\n",
    "    - Bias, Risks, and Limitations of the distilroberta-base Model: https://huggingface.co/distilroberta-base#bias-risks-and-limitations\n",
    "\n",
    "- Text-Generation Models:\n",
    "    - HuggingFace Guide on Text Generation Tasks: https://huggingface.co/tasks/text-generation\n",
    "    - gpt2 Model Card: https://huggingface.co/gpt2?text=Once+upon+a+time%2C\n",
    "    - ChatGPT-at-Home GitHub Repository: https://github.com/Sentdex/ChatGPT-at-Home/blob/main/app.py\n",
    "\n",
    "**Companion Files:**\n",
    "\n",
    "- Public Repository: [Intro-to-Pygame-and-AI](https://github.com/de-fellows/Intro-to-Pygame-and-AI/tree/main)\n",
    "    - A pygame-based video game with AI: [ai_game.py](https://github.com/de-fellows/Intro-to-Pygame-and-AI/blob/main/ai_game.py)\n",
    "    - Natural language processing models used in ai_game.py: [chat_models.py](https://github.com/de-fellows/Intro-to-Pygame-and-AI/blob/main/chat_models.py)\n",
    "    - Intro to Pygame tutorial: [Intro to Pygame: Pygame basics for your first video game!](https://de-fellows.github.io/RexCoding/python/pygame/2023/06/21/Intro-to-Pygame.html)\n",
    "    - A simple pygame example: [simple_pygame.py](https://github.com/de-fellows/Intro-to-Pygame-and-AI/blob/main/simple_pygame.py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-game-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
